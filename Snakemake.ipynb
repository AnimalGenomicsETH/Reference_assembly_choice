{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary files for running Snakemake\n",
    "\n",
    "The necessary files for running Snakemake are:\n",
    "- [Snakefile](Snakemake/Snakefile.py)\n",
    "\n",
    "**Snakefile** is the file where the different steps are designed and concatenated, the core code.\n",
    "- [Cluster file](Snakemake/config.yaml)\n",
    "\n",
    "Although Snakemake is able to generate the necessary jobs and submit them to the cluster, minimal guides need to be prepared: general rules but also specific rules for each job. The **cluster.json** serves for this purpose. Running time, number of cores used, memory required, name of the job and log locations are the parameters to be set within this JSON file.\n",
    "- [Config file](Snakemake/cluster.json)\n",
    "\n",
    "The **config.yaml** file has been created in order to make the variables independent from the code, so the Snakefile does not need to change when these variables change. For instance, input files, paths, softwares or wildcards are specified at the YAML file. This way, the core code (Snakefile) remain unchanged from one project to the other: the new user will only need to replace these variables.\n",
    "\n",
    "\n",
    "### Submit all jobs via Snakemake\n",
    "\n",
    "Once the different steps are designed and concatenated in a Snakefile and the variables are given as a config file, it is time for submitting all the jobs and let the cluster do the work. Running jobs with Snakemake in the cluster is a bit different from running the jobs locally, as Snakemake is a workflow manager that automatically distributes the loads and is able to submit the jobs by taking the parameters given as part of the cluster file. \n",
    "\n",
    "As the whole pipeline may take several hours, we need to run the job as \"screen\". Otherwise, either Snakemake or the cluster would close letting the job finish incomplete.\n",
    "\n",
    "Snakefile can be run in screen as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#screen - control a + control d to deattach / screen -r to re-attach\n",
    "module load python_gpu/3.6.4 #python is loaded, otherwise snakemake will not work\n",
    "snakemake --snakefile Snakefile --jobs 999 --cluster-config cluster.json --cluster \"bsub -J {cluster.jobname} -n {cluster.ncore} -W {cluster.jobtime} -o {cluster.logi} -R \\\"rusage[mem={cluster.memo}]\\\"\" \n",
    "#running Snakefile with cluster configuration where job name, time, memory, cores and log destination are defined. \n",
    "#999 is the maximum of jobs that will be created. This number can be increased or reduced depending on the expected number of jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements before running the Snakemake\n",
    "\n",
    "* a reference_path including the reference genome is necessary for the alignment step. Not only the fasta (.fa) file is required (as seen in the command) but also the complimentary files such as the indices (fai) and the ones listed below. It is important that the names are changed so they follow the necessary pattern for Snakefile to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-r-xr-x--- 1 avillas hest-hpc-tg 2.7G Jan 13 12:14 /cluster/work/pausch/temp_scratch/audald/ref/UCD_ref.fa\n",
      "-r-xr-x--- 1 avillas hest-hpc-tg  82K Jan 13 18:49 /cluster/work/pausch/temp_scratch/audald/ref/UCD_ref.fa.fai\n",
      "-r-xr-x--- 1 avillas hest-hpc-tg 309K Jan 13 18:57 /cluster/work/pausch/temp_scratch/audald/ref/UCD_ref.dict\n",
      "-r-xr-x--- 1 avillas hest-hpc-tg 365K Jan 13 18:57 /cluster/work/pausch/temp_scratch/audald/ref/UCD_ref.fa.ann\n",
      "-r-xr-x--- 1 avillas hest-hpc-tg 6.5K Jan 13 18:57 /cluster/work/pausch/temp_scratch/audald/ref/UCD_ref.fa.amb\n",
      "-r-xr-x--- 1 avillas hest-hpc-tg 321K Jan 13 18:57 /cluster/work/pausch/temp_scratch/audald/ref/UCD_ref.fa.NAMES\n",
      "-r-xr-x--- 1 avillas hest-hpc-tg 2.6G Jan 13 18:57 /cluster/work/pausch/temp_scratch/audald/ref/UCD_ref.fa.bwt\n",
      "-r-xr-x--- 1 avillas hest-hpc-tg 658M Jan 13 18:57 /cluster/work/pausch/temp_scratch/audald/ref/UCD_ref.fa.pac\n",
      "-r-xr-x--- 1 avillas hest-hpc-tg 1.3G Jan 13 18:57 /cluster/work/pausch/temp_scratch/audald/ref/UCD_ref.fa.sa\n"
     ]
    }
   ],
   "source": [
    "ls /cluster/work/pausch/temp_scratch/audald/ref/UCD_ref* -lrth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All logs are being written in specific folders in log_folder, as described in the cluster JSON file. Create folder \"log_folder\" and the following sub-folders, one per rule, before starting the submission. The logs will be otherwise be lost or sent to your e-mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignment    fastp\t      sambamba_flagstat_1  sambamba_merge  split_fastq\n",
      "build_index  mark_duplicates  sambamba_flagstat_2  sambamba_sort\n"
     ]
    }
   ],
   "source": [
    "ls /cluster/work/pausch/temp_scratch/audald/best_assembly/log_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips and lessons learnt\n",
    "\n",
    "Tips and lessons learnt are being continously added to [\"How_I_did_stuff\" notebook](https://github.com/Audald/ETH_Jupyter/blob/master/Catch_all/How_I_did_stuff.ipynb).\n",
    "\n",
    "However, some notes I found interesting to document here are:\n",
    "* expand() should be only used in the rule all\n",
    "* \"\\t\" had to be replaced by \"\\\\t\" in the samblaster call\n",
    "* \"\\n\" should be added at the end of the commands, before piping, so each command is separated\n",
    "* Plus signs can be added when piping in th shell from one command to the other\n",
    "\n",
    "### Potential improvement for the alignment workflow\n",
    "\n",
    "* Remove intermediate files/folders and rename the final files\n",
    "* Samples are currently given as a python list contained in config.yaml. It would be amazing to integrate the R/bash/Python scripts I use for fetching the animnals of interest in the workflow.\n",
    "* Depending on the number of read-groups per sample, there will be different sizes for the read-group specific files. While big files take all the resources, tiny ones are underusing them. It would be great to consider a solution in order to optimise resources per sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
